{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7209aae8",
   "metadata": {},
   "source": [
    "# Caching and local datasets\n",
    "Datasets accessed via the intake catalog can be either downloaded into a temporary folder, from where they will be deleted after restarting python, or permanently into a specified directory. If the dataset is already contained within the specified directory, intake will load the data from the local source, instead of downloading it again from the remote server. This is recommended for large datasets or datasets which are used regularily.\n",
    "\n",
    "The following example shows, how to supply local directories to intake using the `simplecache` functionality. Directories of many local datasets are suggested to be stored in a single `.yaml` file to avoid the specification of local directories within the python routines.\n",
    "\n",
    "## Loading the intake catalog\n",
    "We load the intake catalog from ac3airborne, which contains paths to the remote servers, where files are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d0323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ac3airborne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec5240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ac3airborne.get_intake_catalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c4bc6",
   "metadata": {},
   "source": [
    "Additionally we lead the flight-phase-seperation, which contains information on every research flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e889469",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = ac3airborne.get_flight_segments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b27f19",
   "metadata": {},
   "source": [
    "## Example: Dropsonde data\n",
    "The caching functionality will be demonstrated with the dropsonde data published online on the PANGAEA data base. The file of the dataset on PANGAEA is contained in the intake catalog.\n",
    "\n",
    "### Option 1: Download into temporary folder\n",
    "The download into the temporary folder is the default behaviour. Usually the dataset is stored in the `/tmp` directory. We will download the first dropsonde of `ACLOUD_P5_RF23`. The parameter `i_sonde` describes the dropsonde number during the research flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "ds_dsd = cat['ACLOUD']['P5']['DROPSONDES']['ACLOUD_P5_RF23'](i_sonde=1).to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee15d2",
   "metadata": {},
   "source": [
    "By default, the variable name is not readable. By setting the parameter `same_names` of the `simplecache` group to *True* and supplying it to the `storage_options` parameter, the downloaded file has the same file name as the remote file (i.e. the file on PANGAEA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000964f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = {'simplecache': dict(\n",
    "    same_names=True\n",
    ")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5eddf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dsd = cat['ACLOUD']['P5']['DROPSONDES']['ACLOUD_P5_RF23'](i_sonde=1, storage_options=kwds).to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcafec0d",
   "metadata": {},
   "source": [
    "### Option 2: Permanent download into local directory\n",
    "Under the `storage_options` parameter, we can also specify the local directory of the dataset. The path will be supplied to the `same_names` parameter of the `simplecache` group as shown below. If the remote file is contained in the local directory, the local file will be read. Else, the remote file will be downloaded and stored at the specified location permanently. The next time, the data is imported, intake will use the local file. \n",
    "\n",
    "Here, we will store the data relative to the current working directory at `./data/dropsondes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f5c52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = {'simplecache': dict(\n",
    "    cache_storage='./data/dropsondes', \n",
    "    same_names=True\n",
    ")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ac2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dsd = cat['ACLOUD']['P5']['DROPSONDES']['ACLOUD_P5_RF23'](i_sonde=1, storage_options=kwds).to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1643c77",
   "metadata": {},
   "source": [
    "## Managing directories of multiple datasets\n",
    "Datasets may be stored locally in different directories. Instead of specifying the directory in every python script, we can use one file, where all paths are stored for each dataset. Here, we will use a `yaml` file, as it can be read easily into a python dictionary.\n",
    "\n",
    "The file may be structured like this:\n",
    "```\n",
    "DROPSONDES: '/local/path/to/dropsondes'\n",
    "BROADBAND_IRRADIANCE: '/local/path/to/broadband_irradiance'\n",
    "FISH_EYE: '/local/path/to/fish_eye'\n",
    "```\n",
    "\n",
    "In the following, the data will be downloaded in or used from the local *./data* folder of the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ea0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b034f",
   "metadata": {},
   "source": [
    "Now we read the pre-defined yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136c975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DROPSONDES': './data/dropsondes', 'BROADBAND_IRRADIANCE': './data/broadband_irradiance', 'FISH_EYE': './data/fish_eye'}\n"
     ]
    }
   ],
   "source": [
    "with open('./local_datasets.yaml', 'r') as f:\n",
    "    local_dir = yaml.safe_load(f)\n",
    "print(local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194f4f69",
   "metadata": {},
   "source": [
    "As a test, we will download the dropsonde data from ACLOUD RF05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f54945",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'DROPSONDES'\n",
    "flight_id = 'ACLOUD_P5_RF05'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a56f709",
   "metadata": {},
   "source": [
    "We can access the directory, where the data is stored using the dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de94ede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dropsondes\n"
     ]
    }
   ],
   "source": [
    "print(local_dir[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedfded",
   "metadata": {},
   "source": [
    "We add the path now to the `storage_options` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09158763",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = {'simplecache': dict(\n",
    "    cache_storage=local_dir[dataset], \n",
    "    same_names=True\n",
    ")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591ce84",
   "metadata": {},
   "source": [
    "Now we download or use the local dropsonde file. Afterwards, check if the directory `./data/dropsondes` has been created and contains the file `DS_ACLOUD_Flight_05_20170525_V2.nc`. If the directory and the file already exist, the local file will be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e92692f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = cat['ACLOUD']['P5'][dataset][flight_id](i_sonde=1, storage_options=kwds).to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bda7cb",
   "metadata": {},
   "source": [
    "## Get data for offline usage\n",
    "The following example presents a way to download all the data of a certain instrument for all research flights for offline usage. At first, we get all the flights, for which data of the instrument is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "381537eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACLOUD_P5_RF04', 'ACLOUD_P5_RF05', 'ACLOUD_P5_RF06', 'ACLOUD_P5_RF07', 'ACLOUD_P5_RF08', 'ACLOUD_P5_RF10', 'ACLOUD_P5_RF11', 'ACLOUD_P5_RF15', 'ACLOUD_P5_RF17', 'ACLOUD_P5_RF18', 'ACLOUD_P5_RF19', 'ACLOUD_P5_RF20', 'ACLOUD_P5_RF21', 'ACLOUD_P5_RF22', 'ACLOUD_P5_RF23', 'ACLOUD_P5_RF25', 'AFLUX_P5_RF03', 'AFLUX_P5_RF04', 'AFLUX_P5_RF05', 'AFLUX_P5_RF06', 'AFLUX_P5_RF07', 'AFLUX_P5_RF08', 'AFLUX_P5_RF09', 'AFLUX_P5_RF10', 'AFLUX_P5_RF11', 'AFLUX_P5_RF12', 'AFLUX_P5_RF13', 'AFLUX_P5_RF14', 'AFLUX_P5_RF15', 'MOSAiC-ACA_P5_RF05', 'MOSAiC-ACA_P5_RF06', 'MOSAiC-ACA_P5_RF07', 'MOSAiC-ACA_P5_RF08', 'MOSAiC-ACA_P5_RF09', 'MOSAiC-ACA_P5_RF10', 'MOSAiC-ACA_P5_RF11']\n"
     ]
    }
   ],
   "source": [
    "dataset = 'DROPSONDES'\n",
    "flight_ids = []\n",
    "for campaign in ['ACLOUD', 'AFLUX', 'MOSAiC-ACA']:\n",
    "    flight_ids.extend(list(cat[campaign]['P5']['MiRAC-A']))\n",
    "print(flight_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c56967",
   "metadata": {},
   "source": [
    "Now we simply loop over all the flights. In the case of the dropsondes, we set the `i_sonde` parameter to 1, since the dropsondes of the flights are contained all in the same file. This file will be downloaded, if it is not already contained in the directories. Dropsonde datasets are stored in seperate folders for every campaign. The paths, to where the data is downloaded is written into an empty yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b3f9d5c-b0aa-4378-bd22-ab3d2ba9ade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACLOUD_P5_RF05',\n",
       " 'ACLOUD_P5_RF06',\n",
       " 'ACLOUD_P5_RF07',\n",
       " 'ACLOUD_P5_RF10',\n",
       " 'ACLOUD_P5_RF11',\n",
       " 'ACLOUD_P5_RF13',\n",
       " 'ACLOUD_P5_RF14',\n",
       " 'ACLOUD_P5_RF16',\n",
       " 'ACLOUD_P5_RF17',\n",
       " 'ACLOUD_P5_RF18',\n",
       " 'ACLOUD_P5_RF19',\n",
       " 'ACLOUD_P5_RF20',\n",
       " 'ACLOUD_P5_RF21',\n",
       " 'ACLOUD_P5_RF22',\n",
       " 'ACLOUD_P5_RF23']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cat['ACLOUD']['P5'][dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a85fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {dataset: {}}\n",
    "\n",
    "for flight_id in list(cat['ACLOUD']['P5'][dataset]):\n",
    "    \n",
    "    # get mission from flight_id segmentation\n",
    "    mission = flight_id.split('_')[0]\n",
    "    \n",
    "    # define the path, to where data should be downloaded\n",
    "    path = f'./data/{dataset.lower()}/{mission.lower()}'\n",
    "    \n",
    "    # store the path in a dictionary\n",
    "    dct[dataset].update({mission: path})\n",
    "    \n",
    "    # define the parameters for caching\n",
    "    kwds = {'simplecache': dict(\n",
    "        cache_storage=path, \n",
    "        same_names=True\n",
    "    )}\n",
    "    \n",
    "    # read the data to store it\n",
    "    cat['ACLOUD']['P5'][dataset][flight_id](i_sonde=1, storage_options=kwds).to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57e7bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of paths, where data is downloaded to\n",
    "with open('./local_datasets_2.yaml', 'w') as f:\n",
    "    yaml.dump(dct, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
